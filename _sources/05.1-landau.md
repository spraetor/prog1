# Komplexität

Wir wollen die Zeit, die ein Algorithmus zur Lösung eines Problems benötigt,
unabhängig von der Art und Weise seiner praktischen Umsetzung quantifizieren.
Dazu untersuchen wir, wie sich der Aufwand, das heißt die Anzahl der
benötigten Operationen wie arithmetischen Operationen oder Vergleichen,
verändert, wenn wir die Problemgröße verändern.

```{prf:defintion} Problemgröße und Aufwand
:label: dfn-komplexity

Die **(Problem-) Größe** einer Aufgabe ist eine charakteristische Größe des
Problems wie die Anzahl der zu bearbeitenden Daten, die Anzahl von Summanden
oder der Index eines zu berechnenden Folgeglieds.

Der **Aufwand** eines Algorithmus zur Lösung einer Aufgabe der Größe
$n$ ist die Anzahl $\mathcal{A}(n)$ der erforderlichen arithmetischen Operationen und
Vergleiche.
```


## Asymptotisches Verhalten

Anstelle zu zählen wie häufig exakt eine Operation durchgeführt wird, begnügt man sich mit dem Wissen über eine obere Schranke der Häufigkeit, in Abhängigkeit der größe der Eingabedaten. Solch eine Schranke wird mit Hilfe der $\mathcal{O}$-Notation (= "groß Oh") beschrieben:

```{prf:definition}
:label: def-gross-Oh-notation

Sei $g\colon\mathbb{N}_0\to\mathbb{N}_0$, dann bezeichnet

$$
\mathcal{O}(g) := \big\{ f\colon\mathbb{N}_0\to\mathbb{N}_0\,\mid\,\exists c > 0\text{ und }\exists n_0\in\mathbb{N}_0\text{ mit }f(n)\leq c\cdot g(n)\;\forall n\geq n_0 \big\}.
$$

Man schreibt auch $f\in\mathcal{O}(g)$ und sagt "$f$ ist von der Größenordnung groß Oh von g".
```

Nehmen wir an, wir ermitteln die Rechenzeit $T(n)$ für einen bestimmten Algorithmus. Die Variable $n$
kann z. B. die Anzahl der Ein- und Ausgabewerte sein, ihre Summe, oder auch die Größe eines dieser
Werte. Da $T(n)$ maschinenabhängig ist, genügt eine Analyse durch Ausführung des Algorithmus und Messen
der Zeit nicht. Jedoch kann man mit Hilfe einer *a priori Analyse* ein $g(n)$ bestimmen, so dass $T(n) \in \mathcal{O}(g(n))$.

Wenn wir sagen, dass ein Algorithmus eine Rechenzeit $\mathcal{O}(g(n))$ hat, dann meinen wir damit Folgendes:
Wenn der Algorithmus auf unterschiedlichen Computern mit den gleichen Datensätzen läuft, und
diese die Größe $n$ haben, dann werden die resultierenden Laufzeiten immer kleiner sein als eine Konstante
mal $g(n)$. Bei der Suche nach der Größenordnung von $T(n)$ werden wir darum bemüht sein, das "kleinste" $g(n)$
zu finden, so dass $T(n) \in \mathcal{O}(g(n))$ gilt.

Ist $f(n)=p(n)$ z.B. ein Polynom, so gilt:

```{prf:theorem} Polynomielle Komplexität
:label: thm-complexity-polynom

Für ein Polynom $p(n) = a_0 + a_1 n + a_2 n^2 + \cdots + a_m n^{m}$ vom Grade $m$ gilt: $p(n)\in\mathcal{O}(n^m)$.
```

```{figure} images/asymptotisches-verhalten.png

Visualisierung der (a) oberen und (B) unteren Schranken $\mathcal{O}, \Omega$, sowie (c) der Einschließung $\Theta$. Der Punkt $n_0$ besagt immer, dass erst ab einem gewissen Wert die Ungleichung gelten muss, dann aber für alle $n\geq n_0$. (Quelle: {cite:p}`CormenEtAl2022Introduction`)
```
