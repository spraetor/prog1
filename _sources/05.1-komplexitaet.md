# Komplexität

Wir wollen die Zeit, die ein Algorithmus zur Lösung eines Problems benötigt,
unabhängig von der Art und Weise seiner praktischen Umsetzung quantifizieren.
Dazu untersuchen wir, wie sich der Aufwand, das heißt die Anzahl der
benötigten Operationen wie arithmetischen Operationen oder Vergleichen,
verändert, wenn wir die Problemgröße verändern.

```{prf:defintion} Problemgröße und Aufwand
:label: dfn-komplexity

Die **(Problem-) Größe** einer Aufgabe ist eine charakteristische Größe des
Problems wie die Anzahl der zu bearbeitenden Daten, die Anzahl von Summanden
oder der Index eines zu berechnenden Folgeglieds.

Der **Aufwand** eines Algorithmus zur Lösung einer Aufgabe der Größe
$n$ ist die Anzahl $\mathcal{A}(n)$ der erforderlichen arithmetischen Operationen und
Vergleiche.
```


## Asymptotisches Verhalten

Anstelle zu zählen wie häufig exakt eine Operation durchgeführt wird, begnügt man sich mit dem Wissen über eine obere Schranke der Häufigkeit, in Abhängigkeit der größe der Eingabedaten. Solch eine Schranke wird mit Hilfe der $\mathcal{O}$-Notation (= "groß Oh") beschrieben:

```{prf:definition}
:label: def-gross-Oh-notation

Sei $g\colon\mathbb{N}\to\mathbb{N}$, dann bezeichnet

$$
\mathcal{O}(g) := \big\{ f\colon\mathbb{N}\to\mathbb{N}\,\mid\,\exists c > 0\text{ und }\exists n_0\in\mathbb{N}\text{ mit }f(n)\leq c\cdot g(n)\;\forall n\geq n_0 \big\}.
$$

Man schreibt auch $f\in\mathcal{O}(g)$ und sagt "$f$ ist von der Größenordnung groß Oh von g", bzw. $f$ wächst höchstens so schnell wie $g$ wenn $n\to\infty$.
```

Nehmen wir an, wir ermitteln die Rechenzeit $T(n)$ für einen bestimmten Algorithmus. Die Variable $n$
kann z. B. die Anzahl der Ein- und Ausgabewerte sein, ihre Summe, oder auch die Größe eines dieser
Werte. Da $T(n)$ maschinenabhängig ist, genügt eine Analyse durch Ausführung des Algorithmus und Messen
der Zeit nicht. Jedoch kann man mit Hilfe einer *a priori Analyse* ein $g(n)$ bestimmen, so dass $T(n) \in \mathcal{O}(g(n))$.

Wenn wir sagen, dass ein Algorithmus eine Rechenzeit $\mathcal{O}(g(n))$ hat, dann meinen wir damit Folgendes:
Wenn der Algorithmus auf unterschiedlichen Computern mit den gleichen Datensätzen läuft, und
diese die Größe $n$ haben, dann werden die resultierenden Laufzeiten immer kleiner sein als eine Konstante
mal $g(n)$, zumindest wen $n$ groß genug ist. (deswegen "asymptotisches Verhalten") Bei der Suche nach der
Größenordnung von $T(n)$ werden wir darum bemüht sein, das "kleinste" $g(n)$
zu finden, so dass $T(n) \in \mathcal{O}(g(n))$ gilt.


Ist $f(n)=p(n)$ z.B. ein Polynom, so gilt:

```{prf:theorem} Polynomielle Komplexität
:label: thm-complexity-polynom

Für ein Polynom $p(n) = a_0 + a_1 n + a_2 n^2 + \cdots + a_m n^{m}$ vom Grade $m$ gilt: $p(n)\in\mathcal{O}(n^m)$.
```

Wenn es eine "groß Oh"-Notation gibt, dann auch eine "klein Oh"-Notation.

```{prf:definition}
:label: def-klein-Oh-notation

Sei $g\colon\mathbb{N}\to\mathbb{N}$, dann bezeichnet

$$
\mathcal{o}(g) := \big\{ f\colon\mathbb{N}\to\mathbb{N}\,\mid\,\forall c > 0\,\exists n_0\in\mathbb{N}\text{ mit }f(n) < c\cdot g(n)\;\forall n\geq n_0 \big\}.
$$

Man schreibt auch $f\in\mathcal{o}(g)$ und sagt "$f$ ist von der Größenordnung klein Oh von g", bzw. $f$ wächst langsamer als $g$ wenn $n\to\infty$.
```

Das Landau-Symbol $\mathcal{o}$ läßt uns Größenordnungen unterscheiden. Wir können für zwei beliebige Funktionen
$f(n)$, $g(n)$ entscheiden, ob sie gleich schnell wachsen, oder welche Funktion schneller wächst, also eine größere
Größenordnung hat. Deshalb führen wir eine Ordnung der Größenordnungen ein, in dem wir $\mathcal{O}(f(n)) < \mathcal{O}(g(n))$
schreiben, falls $f \in \mathcal{o}(g)$ ist .Für einige wichtige Funktionen, die bei der Laufzeitanalyse des öfteren
eine Rolle spielen, ist diese Ordnung durch

$$
\mathcal{O}(1) < \mathcal{O}(\log n) < \mathcal{O}(n) < \mathcal{O}(n \log n) < \mathcal{O}(n^2) < \mathcal{O}(n^{\log n}) < \mathcal{O}(2^n)
$$

gegeben.

```{exercise}
Beweise diese Abschätzungen!
```

Zur Illustration wichtiger Größenordnungen sind in folgender Tabelle einige Werte von $n$ in die entsprechenden Funktionen eingesetzt.

```{table} Wachstum verschiedener Größenordnungen
:name: tbl-groessenordnungen
:widths: auto
:align: center

| $\log{n}$| $n$ | $n\log{n}$ | $n^2$ | $n^{\log n}$ | $2^n$ |
| -------- | --- | ---------- | ----- | ----- | ----- |
| 0        | 1   | 0          | 1     | 1     | 2     |
| 1        | 2   | 2          | 4     | 2     | 4     |
| 2        | 4   | 8          | 16    | 16    | 16    |
| 3        | 9   | 24         | 64    | 512   | 256   |
| 4        | 16  | 64         | 256   | 65536  | 65536 |
| 5        | 32  | 160        | 1024  | $\approx 3.4\cdot 10^7$ | $\approx 4.3\cdot 10^9$ |
| 10       | 1024| 10240      | 1048576 | $\approx 1.3\cdot 10^{30}$ | $\approx 1.8\cdot 10^{308}$ |
```



```{figure} images/asymptotisches-verhalten.png

Visualisierung der (a) oberen und (B) unteren Schranken $\mathcal{O}, \Omega$, sowie (c) der Einschließung $\Theta$. Der Punkt $n_0$ besagt immer, dass erst ab einem gewissen Wert die Ungleichung gelten muss, dann aber für alle $n\geq n_0$. (Quelle: {cite:p}`CormenEtAl2022Introduction`)
```
